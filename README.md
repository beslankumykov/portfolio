# Portfolio

| Название проекта | Задачи проекта | Стек технологий |
| :- | :- | :- |
| 1. Сегментация целевой аудитории для агрегатора доставки еды | 1. Выяснить требования к целевой витрине. <br/> 2. Изучить структуру исходных данных. <br/> 3. Подготовить витрину. <br/> 4. Доработать представления. | PostgreSQL SQL | 
| 2. Оптимизация модели данных интернет-магазина и сборка витрины | 1. Разложить данные из одной таблицы по нескольким логическим таблицам. <br/> 2. Создать удобное представление данных на основе новых логических таблиц. | PostgreSQL SQL |
| 3. Расчет клиентских метрик для интернет-магазина| 1. Адаптировать пайплайн для текущей задачи: учесть в витрине нужные статусы и обновить пайплайн с учётом этих статусов. <br/> 2. На основе пайплайна наполнить витрину данными по «возвращаемости клиентов» в разрезе недель. <br/> 3. Перезапустить пайплайн и убедиться, что после перезапуска не появилось дубликатов в витринах. | Airflow PostgreSQL Python REST-API S3 |
| 4. Реализация витрины для расчётов выплат курьерам | 1. Изучить API системы доставки заказов. <br/> 2. Спроектировать структуру таблиц для слоёв в хранилище. <br/> 3. Реализовать DAG. | Airflow MongoDB PostgreSQL Python REST-API |
| 5. Поиск сообществ с высокой конверсией в первое сообщение | 1. Перенести из S3 в staging-слой новые данные о входе и выходе пользователей из групп. <br/> 2. Создать в слое постоянного хранения таблицы для новых данных. <br/> 3. Перенести новые данные из staging-области в слой DDS. <br/> 4. Рассчитать конверсионные показатели для десяти самых старых сообществ. | Airflow PostgreSQL REST-API Vertica S3 |
| 6. Обновление структуры Data Lake соцсети и сборка трех витрин | 1. Обновить структуру Data Lake. <br/> 2. Создать витрину в разрезе пользователей. <br/> 3. Создать витрину в разрезе зон. <br/> 4. Создать витрину рекомендаций друзей. <br/> 5. Автоматизировать обновление витрин. | Apache Spark HDFS Hadoop MapReduce |
| 7. Настройка потоковой обработки данных для агрегатора доставки еды | Написать сервис, который будет: <br/> 1. Читать данные из Kafka с помощью Spark Structured Streaming и Python в режиме реального времени. <br/> 2. Получать список подписчиков из базы данных Postgres. <br/> 3. Джойнить данные из Kafka с данными из БД. <br/> 4. Сохранять в памяти полученные данные, чтобы не собирать их заново после отправки в Postgres или Kafka. <br/> 5. Отправлять выходное сообщение в Kafka с информацией об акции, пользователе со списком избранного и ресторане. <br/> 6. Вставлять записи в Postgres, чтобы получить фидбэк от пользователя. | Kafka PostgreSQL PySpark Python Spark Streaming |
| 8. Создание микросервисов обработки данных для агрегатора доставки еды | 1. С помощью Kubernetes написать сервис для наполнения слоя с сырыми данными. <br/> 2. С помощью Kubernetes написать сервис для наполнения слоя DDS. <br/> 3. С помощью Kubernetes написать сервис для наполнения слоя с витринами. <br/> 4. Сделать визуализацию для аналитиков по популярности блюд. | DataLens Yandex Cloud PostgreSQL Docker Python SQL Kafka Redis |


```python 
print("Это фрагмент кода на python")
```
[Подробнее о Markdown по ссылке](https://daringfireball.net/projects/markdown/)
