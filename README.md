# Portfolio

| Название проекта | Задачи проекта | Стек технологий |
| :- | :- | :- |
| 1. Создание витрины данных для сегментации целевой аудитории в приложении агрегатора доставки еды | 1. Выяснить требования к целевой витрине. 2. Изучить структуру исходных данных. 3. Подготовить витрину. 4. Доработать представления. | PostgreSQL SQL | 
| 2. Оптимизация модели данных интернет-магазина и сборка витрины данных | 1. Разложить данные из одной таблицы по нескольким логическим таблицам. 2. Создать удобное представление данных на основе новых логических таблиц. | PostgreSQL SQL |
| 3. Обновление пайплайна обработки данных для интернет-магазина и расчет клиентских метрик | 1. Адаптировать пайплайн для текущей задачи: учесть в витрине нужные статусы и обновить пайплайн с учётом этих статусов. 2. На основе пайплайна наполнить витрину данными по «возвращаемости клиентов» в разрезе недель. 3. Перезапустить пайплайн и убедиться, что после перезапуска не появилось дубликатов в витринах. | Airflow PostgreSQL Python REST-API S3 |
| 4. Реализация витрины для расчётов выплат курьерам | 1. Изучить API системы доставки заказов. 2. Спроектировать структуру таблиц для слоёв в хранилище. 3. Реализовать DAG. | Airflow MongoDB PostgreSQL Python REST-API |
| 5. Поиск сообществ с высокой конверсией в первое сообщение | 1. Перенести из S3 в staging-слой новые данные о входе и выходе пользователей из групп. 2. Создать в слое постоянного хранения таблицы для новых данных. 3. Перенести новые данные из staging-области в слой DDS. 4. Рассчитать конверсионные показатели для десяти самых старых сообществ. | Airflow PostgreSQL REST-API Vertica S3 |
| 6. Обновление структуры Data Lake соцсети и формирование витрины данных с координатами действий пользователей. | 1. Обновить структуру Data Lake. 2. Создать витрину в разрезе пользователей. 3. Создать витрину в разрезе зон. 4. Построить витрину для рекомендации друзей. 5. Автоматизировать обновление витрин. | Apache Spark HDFS Hadoop MapReduce |
| 7. Настройка потоковой обработки данных для агрегатора доставки еды | Написать сервис, который будет: 1. Читать данные из Kafka с помощью Spark Structured Streaming и Python в режиме реального времени. 2. Получать список подписчиков из базы данных Postgres. 3. Джойнить данные из Kafka с данными из БД. 4. Сохранять в памяти полученные данные, чтобы не собирать их заново после отправки в Postgres или Kafka. 5. Отправлять выходное сообщение в Kafka с информацией об акции, пользователе со списком избранного и ресторане. 6. Вставлять записи в Postgres, чтобы получить фидбэк от пользователя. | Kafka PostgreSQL PySpark Python Spark Streaming |
| 8. Создание микросервисов обработки данных с использованием облачных технологий для агрегатора доставки еды | 1. С помощью Kubernetes написать сервис для наполнения слоя с сырыми данными. 2. С помощью Kubernetes написать сервис для наполнения слоя DDS. 3. С помощью Kubernetes написать сервис для наполнения слоя с витринами. 4. Сделать визуализацию для аналитиков по популярности блюд. | DataLens Yandex Cloud PostgreSQL Docker Python SQL Kafka Redis |


```python 
print("Это фрагмент кода на python")
```
[Подробнее о Markdown по ссылке](https://daringfireball.net/projects/markdown/)
